{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyadi\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\liyadi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import ast\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec, doc2vec, Doc2Vec\n",
    "\n",
    "pd.set_option(\"display.max_rows\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression model from tfidf-word, all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_set = pd.read_pickle('full_cleaned.pkl')\n",
    "full_set_sw = pd.read_pickle('full_cleaned_sw.pkl')\n",
    "word2vec_model = Word2Vec.load('models/Myword2vec.model')\n",
    "\n",
    "train = pd.read_csv('data/train.csv', sep=',', encoding='utf8')\n",
    "test = pd.read_csv('data/test.csv', sep=',', encoding='utf8')\n",
    "n_train = train.shape[0]\n",
    "n_test = test.shape[0]\n",
    "labels = train.columns[2:8].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training classifier on toxic label\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 31.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}, cv_score: 0.9686441612554454, train_score: 0.9872585613774817, test_score: 0.9717791800531692\n",
      "Fitting final model and making prediction for submission on toxic label\n",
      "Start training classifier on severe_toxic label\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 31.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}, cv_score: 0.9851474552431835, train_score: 0.9917170363946917, test_score: 0.9833844191221379\n",
      "Fitting final model and making prediction for submission on severe_toxic label\n",
      "Start training classifier on obscene label\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 28.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5e-05}, cv_score: 0.9848137941650619, train_score: 0.9943263376125865, test_score: 0.9831182110502592\n",
      "Fitting final model and making prediction for submission on obscene label\n",
      "Start training classifier on threat label\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 30.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 0.0001}, cv_score: 0.9811428041068667, train_score: 0.9973667522113111, test_score: 0.9736231038582817\n",
      "Fitting final model and making prediction for submission on threat label\n",
      "Start training classifier on insult label\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 29.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5e-05}, cv_score: 0.9761933842973832, train_score: 0.9889081949539981, test_score: 0.9758932102834541\n",
      "Fitting final model and making prediction for submission on insult label\n",
      "Start training classifier on identity_hate label\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 30.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}, cv_score: 0.9727445487435387, train_score: 0.9927667679299728, test_score: 0.9790201632236318\n",
      "Fitting final model and making prediction for submission on identity_hate label\n",
      "Wall time: 3h 11min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_lg_records = pd.DataFrame(index=labels, columns=['best_params', 'cv_score', 'train_score', 'test_score'])\n",
    "word_lg_full = pd.DataFrame(index=full_set.index, columns=labels)\n",
    "for label in labels:\n",
    "    print('Start training classifier on {} label'.format(label))\n",
    "    x_train, x_test, y_train, y_test, train_idx, test_idx = train_test_split(full_set.cleaned_text[:n_train], \n",
    "                                                        full_set[label][:n_train], \n",
    "                                                        np.arange(n_train),\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state=2018,\n",
    "                                                        stratify = full_set[label][:n_train])\n",
    "    word_lg = Pipeline([('tfidf', TfidfVectorizer()), ('lg', LogisticRegression())])\n",
    "    parameters = {'tfidf__max_df': [0.5, 0.4, 0.2],\\\n",
    "                  'tfidf__min_df': [0.00005, 0.0001, 0.0002],\\\n",
    "                  'lg__class_weight': ['balanced'],\\\n",
    "                  'lg__solver': ['lbfgs'],\\\n",
    "                  'lg__C': [0.05, 0.1, 0.2, 0.4, 0.8, 1]\n",
    "                 }\n",
    "    word_lg_cv = GridSearchCV(word_lg, parameters, n_jobs=-1, scoring = 'roc_auc', verbose=1, cv=5)\n",
    "    word_lg_cv.fit(x_train, y_train)\n",
    "    word_lg_records.loc[label,'best_params'] = str(word_lg_cv.best_params_)\n",
    "    word_lg_records.loc[label,'cv_score'] = word_lg_cv.best_score_\n",
    "    word_lg_records.loc[label, 'train_score'] = metrics.roc_auc_score(y_train, word_lg_cv.predict_proba(x_train)[:,1])\n",
    "    word_lg_records.loc[label, 'test_score'] = metrics.roc_auc_score(y_test, word_lg_cv.predict_proba(x_test)[:,1])\n",
    "    print('Best params: {}, cv_score: {}, train_score: {}, test_score: {}'.format(*word_lg_records.loc[label]))\n",
    "    word_lg.set_params(**word_lg_cv.best_params_)\n",
    "    print('Fitting final model and making prediction for submission on {} label'.format(label))\n",
    "    word_lg.fit(full_set.cleaned_text[:n_train], full_set[label][:n_train])\n",
    "    word_lg_full[label] = word_lg.predict_proba(full_set.cleaned_text)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}\n",
      "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}\n",
      "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5e-05}\n",
      "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 0.0001}\n",
      "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5e-05}\n",
      "{'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}\n",
      "               cv_score train_score test_score\n",
      "toxic          0.968644    0.987259   0.971779\n",
      "severe_toxic   0.985147    0.991717   0.983384\n",
      "obscene        0.984814    0.994326   0.983118\n",
      "threat         0.981143    0.997367   0.973623\n",
      "insult         0.976193    0.988908   0.975893\n",
      "identity_hate  0.972745    0.992767    0.97902\n",
      "cv_score       0.978114\n",
      "train_score    0.992057\n",
      "test_score     0.977803\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(*word_lg_records.best_params, sep='\\n')\n",
    "print(word_lg_records.iloc[:,1:])\n",
    "print(word_lg_records.iloc[:,1:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001}\n",
    "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001}\n",
    "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001}\n",
    "{'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001}\n",
    "               cv_score train_score test_score\n",
    "toxic           0.96785     0.98584   0.971031\n",
    "severe_toxic   0.984672    0.991401    0.98346\n",
    "obscene        0.984222    0.993775   0.981968\n",
    "threat         0.981143    0.997367   0.973623\n",
    "insult          0.97526    0.988024   0.975399\n",
    "identity_hate  0.971298    0.992237   0.978517\n",
    "cv_score       0.977407\n",
    "train_score    0.991440\n",
    "test_score     0.977333\n",
    "dtype: float64\n",
    "\n",
    "{'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}\n",
    "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5e-05}\n",
    "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 0.0001}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5e-05}\n",
    "{'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__max_df': 0.5, 'tfidf__min_df': 5e-05}\n",
    "               cv_score train_score test_score\n",
    "toxic          0.968644    0.987259   0.971779\n",
    "severe_toxic   0.985147    0.991717   0.983384\n",
    "obscene        0.984814    0.994326   0.983118\n",
    "threat         0.981143    0.997367   0.973623\n",
    "insult         0.976193    0.988908   0.975893\n",
    "identity_hate  0.972745    0.992767    0.97902\n",
    "cv_score       0.978114\n",
    "train_score    0.992057\n",
    "test_score     0.977803\n",
    "\n",
    "                  'tfidf__max_df': [0.5, 0.4, 0.3, 0.2],\\\n",
    "                  'tfidf__min_df': [0.00005, 0.0001, 0.0002, 0.0003, 0.001, 0.003],\\\n",
    "                  'lg__class_weight': ['balanced'],\\\n",
    "                  'lg__solver': ['lbfgs'],\\\n",
    "                  'lg__C': [0.05, 0.1, 0.2, 0.4, 0.8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_lg_full.to_csv('word_lg_full.csv',index=False)\n",
    "word_lg_records.to_csv('word_lg_records.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **create train test sets for stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start generating out-of-sample predictions on toxic label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on severe_toxic label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on obscene label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on threat label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on insult label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on identity_hate label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_lg_stack_train = pd.DataFrame(np.zeros((n_train, 6)), columns=labels)\n",
    "word_lg_stack_test = pd.read_csv('data/word_lg_full.csv')[n_train:]\n",
    "word_lg_records = pd.read_csv('data/word_lg_records.csv')\n",
    "\n",
    "for j,label in enumerate(labels):\n",
    "    print('Start generating out-of-sample predictions on {} label for stacking'.format(label))\n",
    "    skf = list(StratifiedKFold(full_set[label][:n_train], 5))\n",
    "    word_lg = Pipeline([('tfidf', TfidfVectorizer()), ('lg', LogisticRegression())])\n",
    "    word_lg.set_params(**ast.literal_eval(word_lg_records.loc[j, 'best_params']))\n",
    "    for i,(train, test) in enumerate(skf):\n",
    "        print(\"Fold\", i+1)\n",
    "        X_train = full_set.cleaned_text[train]\n",
    "        y_train = full_set[label][train]\n",
    "        X_test = full_set.cleaned_text[test]\n",
    "        y_test = full_set[label][test]\n",
    "        word_lg.fit(X_train, y_train)\n",
    "        word_lg_stack_train.iloc[test, j] = word_lg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110488</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.040956</td>\n",
       "      <td>0.048775</td>\n",
       "      <td>0.046334</td>\n",
       "      <td>0.033905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101612</td>\n",
       "      <td>0.058227</td>\n",
       "      <td>0.050438</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.027718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217676</td>\n",
       "      <td>0.021963</td>\n",
       "      <td>0.076243</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.085711</td>\n",
       "      <td>0.014672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>0.013808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.132561</td>\n",
       "      <td>0.079294</td>\n",
       "      <td>0.277578</td>\n",
       "      <td>0.039353</td>\n",
       "      <td>0.186736</td>\n",
       "      <td>0.193850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0.049322</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.043899</td>\n",
       "      <td>0.022527</td>\n",
       "      <td>0.028673</td>\n",
       "      <td>0.011398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0.887003</td>\n",
       "      <td>0.032935</td>\n",
       "      <td>0.099163</td>\n",
       "      <td>0.177186</td>\n",
       "      <td>0.135184</td>\n",
       "      <td>0.026931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0.128140</td>\n",
       "      <td>0.081379</td>\n",
       "      <td>0.106372</td>\n",
       "      <td>0.053408</td>\n",
       "      <td>0.191273</td>\n",
       "      <td>0.074452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0.110361</td>\n",
       "      <td>0.034977</td>\n",
       "      <td>0.055945</td>\n",
       "      <td>0.056336</td>\n",
       "      <td>0.107064</td>\n",
       "      <td>0.061265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0.180637</td>\n",
       "      <td>0.042216</td>\n",
       "      <td>0.146344</td>\n",
       "      <td>0.276876</td>\n",
       "      <td>0.138925</td>\n",
       "      <td>0.023616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0       0.110488      0.044466  0.040956  0.048775  0.046334       0.033905\n",
       "1       0.101612      0.058227  0.050438  0.028976  0.061455       0.027718\n",
       "2       0.217676      0.021963  0.076243  0.008420  0.085711       0.014672\n",
       "3       0.002913      0.013508  0.010774  0.010439  0.005893       0.013808\n",
       "4       0.132561      0.079294  0.277578  0.039353  0.186736       0.193850\n",
       "...          ...           ...       ...       ...       ...            ...\n",
       "159566  0.049322      0.036703  0.043899  0.022527  0.028673       0.011398\n",
       "159567  0.887003      0.032935  0.099163  0.177186  0.135184       0.026931\n",
       "159568  0.128140      0.081379  0.106372  0.053408  0.191273       0.074452\n",
       "159569  0.110361      0.034977  0.055945  0.056336  0.107064       0.061265\n",
       "159570  0.180637      0.042216  0.146344  0.276876  0.138925       0.023616\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lg_stack_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_lg_stack_train.to_csv('data/word_lg_stack_train.csv')\n",
    "word_lg_stack_test.to_csv('data/word_lg_stack_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression model with tfidf-char, all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training classifier on toxic label\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 76.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 104.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}, cv_score: 0.9688033831267864, train_score: 0.9923210747108505, test_score: 0.9725522185757541\n",
      "Fitting final model and making prediction for submission on toxic label\n",
      "Start training classifier on severe_toxic label\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 71.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 99.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0003, 'tfidf__ngram_range': (1, 5)}, cv_score: 0.9872511038013121, train_score: 0.993530780682829, test_score: 0.9875079917659509\n",
      "Fitting final model and making prediction for submission on severe_toxic label\n",
      "Start training classifier on obscene label\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 77.2min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 166.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}, cv_score: 0.9886951396500243, train_score: 0.9973444868645687, test_score: 0.988596326368802\n",
      "Fitting final model and making prediction for submission on obscene label\n",
      "Start training classifier on threat label\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 82.1min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 111.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}, cv_score: 0.9761448347308186, train_score: 0.999219692732018, test_score: 0.9760630113558986\n",
      "Fitting final model and making prediction for submission on threat label\n",
      "Start training classifier on insult label\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 87.7min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 123.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}, cv_score: 0.9778383091315479, train_score: 0.9935849276972556, test_score: 0.9778042816335499\n",
      "Fitting final model and making prediction for submission on insult label\n",
      "Start training classifier on identity_hate label\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 87.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 122.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0003, 'tfidf__ngram_range': (1, 5)}, cv_score: 0.9795780395096644, train_score: 0.9965552579050235, test_score: 0.9858488220588821\n",
      "Fitting final model and making prediction for submission on identity_hate label\n",
      "Wall time: 13h 26min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "char_lg_records = pd.DataFrame(index=labels, columns=['best_params', 'cv_score', 'train_score', 'test_score'])\n",
    "char_lg_full = pd.DataFrame(index=full_set.index, columns=labels)\n",
    "for label in labels:\n",
    "    print('Start training classifier on {} label'.format(label))\n",
    "    x_train, x_test, y_train, y_test, train_idx, test_idx = train_test_split(full_set.cleaned_text[:n_train], \n",
    "                                                        full_set[label][:n_train], \n",
    "                                                        np.arange(n_train),\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state=2018,\n",
    "                                                        stratify = full_set[label][:n_train])\n",
    "    char_lg = Pipeline([('tfidf', TfidfVectorizer()), ('lg', LogisticRegression())])\n",
    "    parameters = {'tfidf__analyzer': ['char'],\\\n",
    "              'tfidf__max_df': [0.4, 0.2],\\\n",
    "              'tfidf__min_df': [0.0001, 0.0003],\\\n",
    "              'tfidf__ngram_range': [(1,5)],\\\n",
    "#               'tfidf__max_features': [40000],\\\n",
    "              'lg__class_weight': ['balanced'],\\\n",
    "              'lg__solver': ['lbfgs'],\\\n",
    "              'lg__C': [0.1, 0.2, 0.4, 0.8, 1]\n",
    "                 }\n",
    "    char_lg_cv = GridSearchCV(char_lg, parameters, n_jobs=-1, scoring = 'roc_auc', verbose=1, cv=3)\n",
    "    char_lg_cv.fit(x_train, y_train)\n",
    "    char_lg_records.loc[label,'best_params'] = str(char_lg_cv.best_params_)\n",
    "    char_lg_records.loc[label,'cv_score'] = char_lg_cv.best_score_\n",
    "    char_lg_records.loc[label, 'train_score'] = metrics.roc_auc_score(y_train, char_lg_cv.predict_proba(x_train)[:,1])\n",
    "    char_lg_records.loc[label, 'test_score'] = metrics.roc_auc_score(y_test, char_lg_cv.predict_proba(x_test)[:,1])\n",
    "    print('Best params: {}, cv_score: {}, train_score: {}, test_score: {}'.format(*char_lg_records.loc[label]))\n",
    "    char_lg.set_params(**char_lg_cv.best_params_)\n",
    "    print('Fitting final model and making prediction for submission on {} label'.format(label))\n",
    "    char_lg.fit(full_set.cleaned_text[:n_train], full_set[label][:n_train])\n",
    "    char_lg_full[label] = char_lg.predict_proba(full_set.cleaned_text)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
      "{'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0003, 'tfidf__ngram_range': (1, 5)}\n",
      "{'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
      "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
      "{'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
      "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0003, 'tfidf__ngram_range': (1, 5)}\n",
      "               cv_score train_score test_score\n",
      "toxic          0.968803    0.992321   0.972552\n",
      "severe_toxic   0.987251    0.993531   0.987508\n",
      "obscene        0.988695    0.997344   0.988596\n",
      "threat         0.976145     0.99922   0.976063\n",
      "insult         0.977838    0.993585   0.977804\n",
      "identity_hate  0.979578    0.996555   0.985849\n",
      "cv_score       0.979718\n",
      "train_score    0.995426\n",
      "test_score     0.981395\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(*char_lg_records.best_params, sep='\\n')\n",
    "print(char_lg_records.iloc[:,1:])\n",
    "print(char_lg_records.iloc[:,1:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__max_features': 40000, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__max_features': 40000, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__max_features': 40000, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.4, 'tfidf__max_features': 40000, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.8, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__max_features': 40000, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__max_features': 40000, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "               cv_score train_score test_score\n",
    "toxic          0.967772    0.988358   0.971176\n",
    "severe_toxic   0.987057    0.992343    0.98716\n",
    "obscene        0.988175    0.996573   0.988169\n",
    "threat         0.975422    0.998486   0.976327\n",
    "insult         0.977073    0.991987   0.977095\n",
    "identity_hate  0.979267    0.996085   0.985326\n",
    "cv_score       0.979128\n",
    "train_score    0.993972\n",
    "test_score     0.980875\n",
    "dtype: float64\n",
    "\n",
    "{'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.2, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0003, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.4, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 1, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0001, 'tfidf__ngram_range': (1, 5)}\n",
    "{'lg__C': 0.4, 'lg__class_weight': 'balanced', 'lg__solver': 'lbfgs', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 0.0003, 'tfidf__ngram_range': (1, 5)}\n",
    "               cv_score train_score test_score\n",
    "toxic          0.968803    0.992321   0.972552\n",
    "severe_toxic   0.987251    0.993531   0.987508\n",
    "obscene        0.988695    0.997344   0.988596\n",
    "threat         0.976145     0.99922   0.976063\n",
    "insult         0.977838    0.993585   0.977804\n",
    "identity_hate  0.979578    0.996555   0.985849\n",
    "cv_score       0.979718\n",
    "train_score    0.995426\n",
    "test_score     0.981395\n",
    "\n",
    "              'tfidf__analyzer': ['char'],\\\n",
    "              'tfidf__max_df': [0.4, 0.2],\\\n",
    "              'tfidf__min_df': [0.0001],\\\n",
    "              'tfidf__ngram_range': [(1,5)],\\\n",
    "              'tfidf__max_features': [40000],\\\n",
    "              'lg__class_weight': ['balanced'],\\\n",
    "              'lg__solver': ['lbfgs'],\\\n",
    "              'lg__C': [0.1, 0.2, 0.4, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_lg_full.to_csv('char_lg_full.csv',index=False)\n",
    "char_lg_records.to_csv('char_lg_records.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **create train test sets for stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start generating out-of-sample predictions on toxic label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on severe_toxic label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on obscene label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on threat label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on insult label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on identity_hate label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Wall time: 1h 22min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "char_lg_stack_train = pd.DataFrame(np.zeros((n_train, 6)), columns=labels)\n",
    "char_lg_stack_test = pd.read_csv('data/char_lg_full.csv')[n_train:]\n",
    "char_lg_records = pd.read_csv('data/char_lg_records.csv')\n",
    "\n",
    "for j,label in enumerate(labels):\n",
    "    print('Start generating out-of-sample predictions on {} label for stacking'.format(label))\n",
    "    skf = list(StratifiedKFold(full_set[label][:n_train], 5))\n",
    "    char_lg = Pipeline([('tfidf', TfidfVectorizer()), ('lg', LogisticRegression())])\n",
    "    char_lg.set_params(**ast.literal_eval(char_lg_records.loc[j, 'best_params']))\n",
    "    for i,(train, test) in enumerate(skf):\n",
    "        print(\"Fold\", i+1)\n",
    "        X_train = full_set.cleaned_text[train]\n",
    "        y_train = full_set[label][train]\n",
    "        X_test = full_set.cleaned_text[test]\n",
    "        y_test = full_set[label][test]\n",
    "        char_lg.fit(X_train, y_train)\n",
    "        char_lg_stack_train.iloc[test, j] = char_lg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.155863</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>0.007916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.059573</td>\n",
       "      <td>0.040789</td>\n",
       "      <td>0.020075</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.048695</td>\n",
       "      <td>0.012449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.076517</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.035759</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.040238</td>\n",
       "      <td>0.005473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.002721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072465</td>\n",
       "      <td>0.065547</td>\n",
       "      <td>0.092258</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.037448</td>\n",
       "      <td>0.131524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>0.056251</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.036058</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.019148</td>\n",
       "      <td>0.005189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>0.845648</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.056446</td>\n",
       "      <td>0.087114</td>\n",
       "      <td>0.088171</td>\n",
       "      <td>0.033530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>0.255203</td>\n",
       "      <td>0.051384</td>\n",
       "      <td>0.064337</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>0.212614</td>\n",
       "      <td>0.170811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>0.063541</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.017631</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>0.065390</td>\n",
       "      <td>0.017998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>0.117731</td>\n",
       "      <td>0.024243</td>\n",
       "      <td>0.059450</td>\n",
       "      <td>0.124069</td>\n",
       "      <td>0.037762</td>\n",
       "      <td>0.012786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0       0.155863      0.025108  0.025015  0.021514  0.022704       0.007916\n",
       "1       0.059573      0.040789  0.020075  0.010517  0.048695       0.012449\n",
       "2       0.076517      0.006056  0.035759  0.001399  0.040238       0.005473\n",
       "3       0.002609      0.004936  0.001953  0.001212  0.001212       0.002721\n",
       "4       0.072465      0.065547  0.092258  0.005185  0.037448       0.131524\n",
       "...          ...           ...       ...       ...       ...            ...\n",
       "159566  0.056251      0.009126  0.036058  0.003161  0.019148       0.005189\n",
       "159567  0.845648      0.023545  0.056446  0.087114  0.088171       0.033530\n",
       "159568  0.255203      0.051384  0.064337  0.027378  0.212614       0.170811\n",
       "159569  0.063541      0.010963  0.017631  0.012032  0.065390       0.017998\n",
       "159570  0.117731      0.024243  0.059450  0.124069  0.037762       0.012786\n",
       "\n",
       "[159571 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_lg_stack_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_lg_stack_train.to_csv('data/char_lg_stack_train.csv')\n",
    "char_lg_stack_test.to_csv('data/char_lg_stack_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression model with word2vec, all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load('models/Myword2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_comment_vect(comment_column):\n",
    "    return(np.array([np.mean([word2vec_model.wv.word_vec(word) \n",
    "                              for word in comment.split() if word in word2vec_model.wv.vocab] \n",
    "                             or [np.zeros(300)], axis=0) \n",
    "                     for comment in comment_column]\n",
    "                   ))\n",
    "comment_vect = pd.DataFrame(get_comment_vect(full_set_sw.cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training classifier on toxic label\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.3, 'class_weight': 'balanced', 'solver': 'lbfgs'}, cv_score: 0.9594543530449549, train_score: 0.9621074016531799, test_score: 0.9621607235866694\n",
      "Fitting final model and making prediction for submission on toxic label\n",
      "Start training classifier on severe_toxic label\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}, cv_score: 0.9832881735173025, train_score: 0.9856410432586006, test_score: 0.9820123256743344\n",
      "Fitting final model and making prediction for submission on severe_toxic label\n",
      "Start training classifier on obscene label\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.1, 'class_weight': 'balanced', 'solver': 'lbfgs'}, cv_score: 0.9708923540981752, train_score: 0.9744465417644621, test_score: 0.9700968573959603\n",
      "Fitting final model and making prediction for submission on obscene label\n",
      "Start training classifier on threat label\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}, cv_score: 0.9776276597500672, train_score: 0.9864093870280444, test_score: 0.9736748287186902\n",
      "Fitting final model and making prediction for submission on threat label\n",
      "Start training classifier on insult label\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.3, 'class_weight': 'balanced', 'solver': 'lbfgs'}, cv_score: 0.9669064056247512, train_score: 0.9702868161747173, test_score: 0.9678922162580699\n",
      "Fitting final model and making prediction for submission on insult label\n",
      "Start training classifier on identity_hate label\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}, cv_score: 0.9604158230993391, train_score: 0.9683628270423034, test_score: 0.9646717786642014\n",
      "Fitting final model and making prediction for submission on identity_hate label\n",
      "Wall time: 26min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_lg_records = pd.DataFrame(index=labels, columns=['best_params', 'cv_score', 'train_score', 'test_score'])\n",
    "w2v_lg_full = pd.DataFrame(index=full_set.index, columns=labels)\n",
    "for label in labels:\n",
    "    print('Start training classifier on {} label'.format(label))\n",
    "    x_train, x_test, y_train, y_test, train_idx, test_idx = train_test_split(comment_vect[:n_train], \n",
    "                                                        full_set_sw[label][:n_train], \n",
    "                                                        np.arange(n_train),\n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state=2018,\n",
    "                                                        stratify = full_set_sw[label][:n_train])\n",
    "    w2v_lg = LogisticRegression()\n",
    "    parameters = {'class_weight': ['balanced'],\\\n",
    "                  'solver': ['lbfgs'],\\\n",
    "                  'C': [0.003, 0.01, 0.03, 0.1, 0.3, 1]\n",
    "                 }\n",
    "    w2v_lg_cv = GridSearchCV(w2v_lg, parameters, n_jobs=-1, scoring = 'roc_auc', verbose=5, cv=5)\n",
    "    w2v_lg_cv.fit(x_train, y_train)\n",
    "    w2v_lg_records.loc[label,'best_params'] = str(w2v_lg_cv.best_params_)\n",
    "    w2v_lg_records.loc[label,'cv_score'] = w2v_lg_cv.best_score_\n",
    "    w2v_lg_records.loc[label, 'train_score'] = metrics.roc_auc_score(y_train, w2v_lg_cv.predict_proba(x_train)[:,1])\n",
    "    w2v_lg_records.loc[label, 'test_score'] = metrics.roc_auc_score(y_test, w2v_lg_cv.predict_proba(x_test)[:,1])\n",
    "    print('Best params: {}, cv_score: {}, train_score: {}, test_score: {}'.format(*w2v_lg_records.loc[label]))\n",
    "    w2v_lg.set_params(**w2v_lg_cv.best_params_)\n",
    "    print('Fitting final model and making prediction for submission on {} label'.format(label))\n",
    "    w2v_lg.fit(comment_vect[:n_train], full_set_sw[label][:n_train])\n",
    "    w2v_lg_full[label] = w2v_lg.predict_proba(comment_vect)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.3, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "{'C': 0.1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "{'C': 0.3, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "               cv_score train_score test_score\n",
      "toxic          0.959454    0.962107   0.962161\n",
      "severe_toxic   0.983288    0.985641   0.982012\n",
      "obscene        0.970892    0.974447   0.970097\n",
      "threat         0.977628    0.986409   0.973675\n",
      "insult         0.966906    0.970287   0.967892\n",
      "identity_hate  0.960416    0.968363   0.964672\n",
      "cv_score       0.969764\n",
      "train_score    0.974542\n",
      "test_score     0.970085\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(*w2v_lg_records.best_params, sep='\\n')\n",
    "print(w2v_lg_records.iloc[:,1:])\n",
    "print(w2v_lg_records.iloc[:,1:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec: size=300, window=5\n",
    "{'C': 0.3, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "               cv_score train_score test_score\n",
    "toxic          0.959605    0.962177    0.96255\n",
    "severe_toxic   0.982976    0.985551   0.981858\n",
    "obscene        0.971678    0.974944    0.97064\n",
    "threat         0.977514    0.986768   0.975113\n",
    "insult         0.967502    0.970531   0.969254\n",
    "identity_hate  0.960262    0.968178   0.964152\n",
    "cv_score       0.969923\n",
    "train_score    0.974691\n",
    "test_score     0.970594\n",
    "\n",
    "word2vec: size=100, window=3\n",
    "{'C': 1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 10, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "{'C': 0.01, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
    "               cv_score train_score test_score\n",
    "toxic           0.95375    0.954787   0.957034\n",
    "severe_toxic     0.9822    0.983757   0.982192\n",
    "obscene        0.964333    0.965793   0.966064\n",
    "threat          0.97269    0.979577   0.972365\n",
    "insult         0.963145    0.964357   0.964598\n",
    "identity_hate  0.957263    0.961801   0.959968\n",
    "cv_score       0.965564\n",
    "train_score    0.968345\n",
    "test_score     0.967037\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_lg_full.to_csv('w2v_lg_full.csv',index=False)\n",
    "w2v_lg_records.to_csv('w2v_lg_records.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create train test sets for stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start generating out-of-sample predictions on toxic label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on severe_toxic label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on obscene label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on threat label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on insult label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Start generating out-of-sample predictions on identity_hate label for stacking\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_lg_stack_train = pd.DataFrame(np.zeros((n_train, 6)), columns=labels)\n",
    "w2v_lg_stack_test = pd.read_csv('w2v_lg_full.csv')[n_train:]\n",
    "w2v_lg_records = pd.read_csv('w2v_lg_records.csv')\n",
    "\n",
    "for j,label in enumerate(labels):\n",
    "    print('Start generating out-of-sample predictions on {} label for stacking'.format(label))\n",
    "    skf = list(StratifiedKFold(full_set_sw[label][:n_train], 5))\n",
    "    w2v_lg = LogisticRegression(**ast.literal_eval(w2v_lg_records.loc[j, 'best_params']))\n",
    "    for i,(train, test) in enumerate(skf):\n",
    "        print(\"Fold\", i+1)\n",
    "        X_train = comment_vect.iloc[train,:]\n",
    "        y_train = full_set_sw[label][train]\n",
    "        X_test = comment_vect.iloc[test,:]\n",
    "        y_test = full_set_sw[label][test]\n",
    "        w2v_lg.fit(X_train, y_train)\n",
    "        w2v_lg_stack_train.iloc[test, j] = w2v_lg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_lg_stack_train.to_csv('w2v_lg_stack_train.csv')\n",
    "w2v_lg_stack_test.to_csv('w2v_lg_stack_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
