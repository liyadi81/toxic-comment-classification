{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, concatenate, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gensim.models import Word2Vec\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "# embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "embed_size = 300\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 4452s 31ms/step - loss: 0.0608 - acc: 0.9793 - val_loss: 0.0492 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 4514s 31ms/step - loss: 0.0434 - acc: 0.9836 - val_loss: 0.0462 - val_acc: 0.9829\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "hist = model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 1067s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.72767415e-04,   1.27316889e-06,   4.43745776e-05,\n",
       "          5.07856328e-07,   8.81044707e-06,   4.73976343e-06],\n",
       "       [  5.58910193e-04,   2.18725359e-06,   7.73255306e-05,\n",
       "          1.34787729e-06,   2.09507834e-05,   7.84819258e-06],\n",
       "       [  4.61251801e-03,   6.44041347e-06,   4.83808020e-04,\n",
       "          8.49240678e-06,   1.75300054e-04,   3.63578511e-05],\n",
       "       ..., \n",
       "       [  3.14858393e-03,   9.14110387e-06,   5.46244206e-04,\n",
       "          5.61349680e-06,   1.65505000e-04,   3.78799632e-05],\n",
       "       [  2.82997335e-03,   2.79880032e-06,   2.57812906e-04,\n",
       "          4.02483238e-06,   9.39967140e-05,   1.55896505e-05],\n",
       "       [  1.47885131e-02,   1.71547072e-05,   1.23190391e-03,\n",
       "          5.45882831e-05,   7.36148912e-04,   1.36105242e-04]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_t,batch_size=batch_size,verbose=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98933042470553367"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 1097s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_submit = model.predict(X_te,batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_submit[np.isnan(y_submit)]=0\n",
    "word_LSTM_submission = pd.read_csv('data/sample_submission.csv')\n",
    "word_LSTM_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_submit\n",
    "word_LSTM_submission.to_csv('word_LSTM_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = pd.read_pickle('full_cleaned_sw_trans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_model = Word2Vec.load('models/Myword2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = full_set['cleaned_text'][:n_train].fillna(\"fillna\").values\n",
    "y_train = full_set[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]][:n_train].values\n",
    "X_test = full_set[\"cleaned_text\"][n_train:].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "# y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "# X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_27 (Embedding)        (None, 100, 300)     9000000     input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_25 (Bidirectional (None, 100, 160)     182880      embedding_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 160)          0           bidirectional_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 160)          0           bidirectional_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 320)          0           global_average_pooling1d_23[0][0]\n",
      "                                                                 global_max_pooling1d_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 320)          0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 6)            1926        dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 9,184,806\n",
      "Trainable params: 9,184,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "#     x = word2vec_model.wv.get_keras_embedding(train_embeddings=False)(inp)\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "#     x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "    x = Dropout(0.2)(x)\n",
    "#     x = Dense(50, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.1)(x)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 3041s 21ms/step - loss: 0.0615 - acc: 0.9796 - val_loss: 0.0504 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.980063 \n",
      "\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 3108s 22ms/step - loss: 0.0414 - acc: 0.9843 - val_loss: 0.0460 - val_acc: 0.9830\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.983599 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 1066s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99211544903882742"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_train,batch_size=batch_size,verbose=1)\n",
    "roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 406s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_submit = model.predict(x_test,batch_size=batch_size,verbose=1)\n",
    "y_submit[np.isnan(y_submit)]=0\n",
    "word_GRU_submission = pd.read_csv('data/sample_submission.csv')\n",
    "word_GRU_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_submit\n",
    "word_GRU_submission.to_csv('word_GRU_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "SpatialDropout1D(0.2)\n",
    "Bidirectional(GRU(80, return_sequences=True))\n",
    "\n",
    "loss: 0.0397 - acc: 0.9849 - val_loss: 0.0464 - val_acc: 0.9826\n",
    "val: ROC-AUC - epoch: 2 - score: 0.984000 \n",
    "train: 0.99301532440477003\n",
    "unprocessed text\n",
    "kaggle: 0.9795\n",
    "\n",
    "loss: 0.0391 - acc: 0.9850 - val_loss: 0.0469 - val_acc: 0.9832\n",
    "val: ROC-AUC - epoch: 2 - score: 0.984314\n",
    "train: 0.99322135847382143\n",
    "processed text\n",
    "kaggle: 0.9790\n",
    "\n",
    "loss: 0.0404 - acc: 0.9846 - val_loss: 0.0461 - val_acc: 0.9827\n",
    "val: ROC-AUC - epoch: 2 - score: 0.984590 \n",
    "train: 0.9928273935811841\n",
    "batch_size = 64 (from 32)\n",
    "kaggle: 0.9792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to-do\n",
    "\n",
    "- char level model\n",
    "- processed vs unprocessed text: slightly better validation score, but got slightly lower score on kaggle, not significant\n",
    "- tune batch size: (32 vs 64, 64 slightly better)\n",
    "- use pretrained word vector vs without (word2vec(skip-gram, cbow), glove, etc)\n",
    "- architecture engineering introduce dropout to  reduce overfitting\n",
    "- hyperparameters: max_features, maxlen, embed_size, dropoutratio, number of GRU\n",
    "\n",
    "<br>\n",
    "- language detect\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
